\chapter{Introduction}
\label{chap:introduction}

The search for more computational power leads to a series of challenges.
Quantum tunneling effects and the speed of light influence the size of
transistors and the length of circuits, and compel the development of
multi-core and heterogeneous computer architectures.  Battery life and a
responsible use of natural resources influence circuit frequency and power
consumption, and compel the development of dynamic frequency scaling and
cooling solutions.  Comparably, the selection and configuration of the
algorithms and computer architectures influence the performance they achieve,
and compel the development of automated selection and configuration solutions.

The use of heterogeneous programming models and computer architectures in High
Performance Computing has increased despite the difficulty of optimizing and
configuring legacy code, and of developing new solutions for heterogeneous
computing. Due to the great diversity of programming models and architectures,
there is no single optimization strategy that fits all problems and
architectures.

An optimization solution tailored for a specific problem requires time and
expert knowledge. Using general optimizations is faster and cheaper, often at
the cost of application-specific performance improvements. A possible solution
to this trade-off is the automation of the program optimization process.

The automated selection of algorithms and configuration of programs, referred
to as \textit{autotuning} from now on, casts the program optimization problem
as a search problem. The possible configurations and optimizations of a program
are used to compose a \textit{search space}, and searching is done by
evaluating the impact of each configuration on the initial program.  Using the
increasingly available computing power to measure different versions of a
program, an \textit{autotuner} searches for good selections, configurations,
and optimizations. The measurements must quantify a meaningful metric for the
programmer, such as execution time and memory or power consumption.

From the implementation in a high-level programming language to the instruction
selection in code generation, it is possible to expose optimization and
configuration opportunities in various stages of program development and
execution.  The time to measure the results of an optimization choice depends
on the selected stage and on the metric to be optimized.

\todo[inline,author=Pedro,color=cyan]{Describe variety of heterogeneous
architectures: NUMA, memory centric, ...}

\todo[inline,author=Pedro,color=cyan]{Show some example: (Genetic Algorithm,
Sorting, Compiler Flag, LLVM Pass)?}

Different steps of a program's development and deployment can be optimized.
Some steps can be more expensive to measure and therefore optimize.  For
example, access to certain architectures might be costly and limited, or it may
take a long time to solve certain problem instances.
\todo[inline,author=Pedro,color=cyan]{Examples: GPU \& CPU compilation,
FPGA hardware generation, compilation for supercomputers, ...}

\todo[inline,author=Pedro,color=cyan]{How to autotune:
 Complex systems, such as HACC?;
 Programs for a supercomputer, like the Titan?;
 FPGA compilation?;
 Hardware and software co-design?}

\todo[inline,author=Pedro,color=cyan]{Current autotuning approaches:
 Require multiple expensive evaluations;
 Do not leverage parallel and distributed computing;
 Not flexible to target multiple development and execution steps}

\section{Objectives and Expected Contributions}
\label{sec:contributions}

\todo[inline,author=Pedro,color=cyan]{A new approach must:
 Leverage distributed and parallel computing power;
 Target different steps of development and execution;
 Handle expensive to measure cases efficiently}

\todo[inline,author=Pedro,color=cyan]{Our expected contribution
is advancing the state-of-the-art of autotuning tools,
leveraging distributed and parallel computing to perform
autotuning in a variety of HPC domains}

\todo[inline,author=Pedro,color=cyan]{Briefly describe the
Julia system}

\section{Efforts for a Reproducible and Open Research}

We believe openness and reproducibility are fundamental to the quality of
Computer Science research. We made efforts to ensure our research is
reproducible and open, and we believe these efforts help our conclusions.  In
this section we describe the efforts we made.

To ensure the openness of our research we uploaded the software developed
during and for research to public hosting services. We used version control
tools and made the code available under free software licenses. We also
made available the copyright-free versions of our papers, which is a common
practice in Computer Science. The research we performed is free to be used and
modified by future researchers.

To ensure the reproducibility of our research we included hardware and software
specifications in our reports and papers. We uploaded the code to generate the
data, text, presentations and data visualizations to public hosting services.
This code is also available under free-software licenses. Researchers are able
to reproduce our results if they have access to the architecture and software
we specify.

Unfortunately, not all of our research could be made open. The hardware
implementations for the CPUs, GPUs, FPGAs and most electronic components used
to produce our results are not made available by their manufacturers. Neither
is the software used to generate FPGA hardware from its Verilog specification.
To ensure reproducibility in these cases we disclosed the hardware
vendor-specified names and characteristics, and the versions of closed software
we used.

With these efforts for a reproducible and open research we believe we have
provided a better foundation for our conclusions and expected contributions.

\section{Text Structure}
\label{sec:org}

\todo[inline,author=Pedro,color=cyan]{Describe each chapter (and
the papers attached to end of the document?)}
