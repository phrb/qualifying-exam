\chapter{Autonomous Search \& Autotuning}
\label{chap:autonomous}
\epigraph{\textit{There is nothing like looking, if you want to find something. You
certainly usually find something, if you look, but it is not always quite the
something you were after.}}{--- J.R.R. Tolkien, \textit{The Hobbit}}


\section{Algorithm Selection and Autonomous Solvers}
\label{sec:algselres}

In 1976 John R. Rice published \textit{The Algorithm
Problem}~\cite{rice1976algorithm}, an influential paper where he formulated
abstract models for the problem of selecting effective algorithms for a given
problem. In Rice's framework the objective is to select the best algorithm,
according a performance metric, for a given problem. Consider a \textit{problem
space} $P$, and a problem $x \in P$.

Figure~\ref{fig:riceframe} presents Rice's algorithm selection
framework.

\begin{figure}[htpb]
    \begin{center}
        \includegraphics[width=.35\textwidth]{algorithm-selection}
    \end{center}
    \caption{Rice's framework}
    \label{fig:riceframe}
\end{figure}

\todo[inline,author=Pedro,color=cyan]{Discuss autonomous solvers}
\todo[inline,author=Pedro,color=cyan]{Discuss the choice of autotuning for
this work, given all the possibilities for solving the same problem
presented in this chapter}

\section{On-line Control}
\label{sec:oncontrol}

\subsection{Adaptive Parameter Configuration}
\label{subsec:paramadaptive}

\subsection{Credit Assignment}
\label{subsec:creditassign}

\subsection{Reinforcement Learning}
\label{subsec:reinforce}

\section{Off-line Configuration}
\label{sec:offconfig}

\subsection{Evolutionary Computing}
\label{subsec:evolcomp}

\subsection{Stochastic Local Search}
\label{subsec:searchsls}

\subsection{Machine Learning}
\label{subsec:searchml}

\section{Autotuning}
\label{chap:autotuning}

Rice's conceptual framework~\cite{rice1976algorithm} formed the foundation of
autotuners in various problem domains.  In 1997, the PHiPAC
system~\cite{bilmes1997optimizing} used code generators and search scripts to
automatically generate high performance code for matrix multiplication. Since
then, systems tackled different domains with a diversity of strategies. Whaley
\emph{et al.}~\cite{dongarra1998automatically} introduced the ATLAS project,
that optimizes dense matrix multiplication routines. The
OSKI~\cite{vuduc2005oski} library provides automatically tuned kernels for
sparse matrices. The FFTW~\cite{frigo1998fftw} library provides tuned C
subroutines for computing the Discrete Fourier Transform.


\begin{table}[htpb]
    \centering
    \begin{tabular}{@{}lll@{}}
        \toprule
        System & Domain & Technique \\ \midrule
        OSKI~\cite{vuduc2005oski} & Sparse Linear Algebra & Heuristic + Exhaustive \\
        LGen~\cite{spampinato2014basic} & Small-scale Linear Algebra & Exhaustive + Random \\
        ATLAS~\cite{dongarra1998automatically} & Dense Linear Algebra & Exhaustive \\
        INSIEME~\cite{jordan2012multi} & Compiler & Genetic Algorithm \\
        Petabricks~\cite{ansel2009petabricks} & Domain-Agnostic & Genetic Algorithm\\
        TANGRAM~\cite{chang2016efficient} & Heterogeneous Architectures & Breadth-First Search + Pruning \\
        SPIRAL~\cite{puschel2005spiral} & DSP Algorithms & Pareto Active Learning \\
        GCC Milepost~\cite{fursin2011milepost} & Compiler & Central DB + Machine Learning \\
        Apollo~\cite{beckingsale2017apollo} & GPU kernels & Decision Trees \\
        Active Harmony~\cite{tapus2002active} & Runtime & Nelder-Mead \\
        MASE-BDI~\cite{coelho2016mase} & Environmental Land Change & Distributed Active Harmony~\cite{tapus2002active} \\
        ParamILS~\cite{hutter2009paramils} & Domain-Agnostic & Stochastic Local Search \\
        CLTune~\cite{nugteren2015cltune} & OpenCL kernels & Stochastic Local Search\\
        OpenTuner~\cite{ansel2014opentuner} & Domain-Agnostic & Ensemble \\
        Periscope~\cite{gerndt2017multi} & HPC Applications & Various \\
        \bottomrule
    \end{tabular}
    \caption{Selected autotuning systems, their domains and techniques}
\end{table}

\subsection{Search Algorithms \& Heuristics}

Abdelfattah \emph{et al.}~\cite{abdelfattah2016performance} present an autotuning
strategy using exhaustive evaluation of General Matrix-Matrix Multiply (GEMM)
kernel optimizations for GPUs. They later use this approach to optimize GPU
kernels for large-scale tensor contractions~\cite{abdelfattah2016high}.
Guerreiro \emph{et al.}~\cite{guerreiro2015multi} use search space pruning strategies
and exhaustive evaluation to autotune CUDA GPU kernels.
LGen~\cite{spampinato2014basic} is a compiler for small-scale linear algebra
computation that uses mathematical domain-specific languages to optimize loops
and vectorization. LGen uses exhaustive or random search to test different
optimizations.

Active Harmony~\cite{tapus2002active} provides an API that enables online
autotuning of a program using a variation of the Nelder-Mead simplex
algorithm~\cite{nelder1965simplex}.  MASE-BDI~\cite{coelho2016mase} is a
multi-agent system for the simulation of environmental land change that uses
the Active Harmony API~\cite{tapus2002active} and its search algorithms to
autotune  simulator parameters.  Several simulator configurations are executed
in parallel and communicate their results to a tuning manager using MPI.
CLTune~\cite{nugteren2015cltune} is an autotuning framework for OpenCL kernels
that uses search algorithms such as simulated annealing and particle swarm
optimization.

\subsection{Genetic Algorithms}

In an effort to provide a common representation of multiple parallel
programming models, the INSIEME compiler project~\cite{jordan2012multi}
implements abstractions for OpenMP, MPI and OpenCL, and generates optimized
parallel code for heterogeneous multi-core architectures.

\subsection{Machine Learning}

Tartara and Reghizzi~\cite{tartara2012parallel} use the MapReduce programming
model to improve the performance of machine learning algorithms in the
PetaBricks compiler. Later they presented a machine learning
algorithm~\cite{tartara2013continuous} that learns compiler heuristics using
data gathered after every compilation.  Mametjanov et
al.~\cite{mametjanov2015autotuning} present an autotuning approach that uses
machine learning and selective sampling of the search space to optimize FPGA
hardware design parameters for performance and power.  Hou et
al.~\cite{hou2017auto} present an autotuning framework that uses decision trees
to optimize the sparse matrix-vector multiply kernel for multi and many-core
processors.  Apollo~\cite{beckingsale2017apollo} is an autotuning framework for
input-dependent kernels that uses a decision tree classifier that provides
optimizations that can be selected during runtime.

\subsection{Model-aided}

TuningGenie~\cite{ivanenko2014method} presents an autotuning framework that
uses code annotations and an analytical model for generating autotuners for
parallel programs.  Falsch and Elster~\cite{falch2017machine} build performance
models for OpenCL applications in different target architectures using machine
learning.  The learned statistical model is later used to pick promising
configurations for testing.  Balaprakash \emph{et al.}~\cite{balaprakash2016automomml}
introduce a framework that uses machine learning algorithms to build models for
metrics such as performance and energy consumption, which are used for
multi-objective optimization.  Lang~\cite{lang2017data} reviews work on
autotuning based on analytical performance models, arguing that the approach is
feasible.  Xu \emph{et al.}~\cite{xu2016analytical} introduce an autotuning framework
for loop scheduling in GPUs that uses analytical performance models.

\subsection{Multiple Techniques}

Garvey \emph{et al.}~\cite{garvey2015automatic} present a strategy to autotune OpenCL
GPU kernels using machine learning, heuristics for search space partitioning
and exhaustive search.  PolyMage~\cite{mullapudi2015polymage} is a
domain-specific language and compiler for image processing pipelines. It uses
heuristics and performance models to prune the search space, which is then
explored exhaustively.  Ziegler et
al.~\cite{ziegler2016synthesis,ziegler2016scalable} present an autotuning
system for hardware synthesis parameters that uses genetic and learning
algorithms to explore the hardware design space for real industrial chip
designs from IBM.  Ngoko \emph{et al.}~\cite{ngoko2016automatic} introduce an
autotuning system for $\NP$-hard problems that uses distributed measurements of
configurations and machine learning algorithms aided by randomization,
clustering and set intersection solvers.  Luo \emph{et al.}~\cite{luo2015fast} present
a framework for stencil computations that uses Optimal-Solution Spaces, input
feature extraction and machine learning.

\subsection{Domain-Agnostic Systems}

Some systems provide generic tools that enable the implementation of autotuners
in various domains.  Petabricks~\cite{ansel2009petabricks} is a language and
compiler that enables the expression of program implementation search spaces at
language level.  OpenTuner~\cite{ansel2014opentuner} is a domain-agnostic
autotuning framework that provides ensembles of search techniques for
user-defined program configurations.  The ParamILS
framework~\cite{hutter2009paramils} applies stochastic local search methods for
algorithm configuration and parameter tuning.
TANGRAM~\cite{chang2015tangram,chang2016efficient} is a kernel synthesis
framework for different architecture hierarchies.  It uses composition rules
and breadth-first search to prune and explore the design space.  The Periscope
Tuning Framework
(PTF)~\cite{gerndt2005periscope,gerndt2010automatic,gerndt2017multi} is an
online tool for performance analysis and tuning that has a modular architecture
based on plugins.

\subsubsection{Works using OpenTuner}

Takizawa et al.~\cite{takizawa2017customizable} use
OpenTuner~\cite{ansel2014opentuner} and Xevolver~\cite{takizawa2014xevolver}, a
code transformation framework, to enable the autotuning of code transformations
in applications that were not implemented for autotuning. They present an
autotuning ``scenario template'' that uses compiler directives to inform the
autotuner.  Bosboom \emph{et al.} and Eliahu use OpenTuner to implement a
domain specific language for data-flow programming~\cite{bosboom2014streamjit}
and a framework for recursive parallel algorithm
optimization~\cite{eliahu2015frpa}.  Xu \emph{et al.}~\cite{xu2017parallel}
present an autotuning strategy that uses parallel OpenTuner instances to
autotune the FPGA compilation flow.  Their work dynamically partition the
search space between MPI-communicating OpenTuner instances.  Jayasena \emph{et
al.}~\cite{jayasena2015auto} use OpenTuner to implement an autotuner for a Java
Virtual Machine implementation.

% Where does this go?

Williams~\cite{williams2009optimization} present optimization strategies for
the sparse matrix-vector multiply kernel in multi-core processors.  Williams et
al.~\cite{williams2009roofline} also present Roofline, a visual performance
model for parallel hardware and software that helps in the optimization of
floating-point computations.  Shuchart et al.~\cite{schuchart2017readex}
introduce the READEX (Runtime Exploitation of Application Dynamism for
Energy-efficient eXascale computing) project and its methodology for developing
tools to improve the energy efficiency of High-Performance Computing
applications.

\subsection{OpenTuner}
\label{sec:opentuner}
\todo[inline,author=Pedro,color=cyan]{Discuss OpenTuner's limitations and shortcomings.}

\subsubsection{Context}
\label{sec:context}

\subsubsection{Software Architecture}
\label{sec:arch}

\subsubsection{Parallel and Distributed Programming in OpenTuner}
\label{sec:opentuner-parallel}
\todo[inline,author=Pedro,color=cyan]{Discuss the difficulties and
explain why they motivated the new Julia code.}

\subsection{Search Techniques}
\label{sec:techniques}

\subsubsection{Numerical Methods}
\label{subsec:num}

\subsubsection{Evolutionary Computation}
\label{subsec:tuninevolcomp}

\subsubsection{Stochastic Local Search}
\label{subsec:tuningsls}

\subsubsection{Machine Learning}
\label{subsec:tuningml}

\subsection{Benchmarks}
\label{sec:benchmarks}

\subsubsection{Solvers of NP-Complete Problems}
\label{subsec:np}

\subsubsection{Algorithm Selection and Configuration}
\label{subsec:algsel}

\subsubsection{Compiler Configuration}
\label{subsec:compilerconfig}

\subsubsection{Measurement Time}
\label{subsec:measure}
