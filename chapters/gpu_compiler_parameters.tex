\section{Selecting Compiler Parameters for GPUs}
\label{sec:paramSelGPU}

A Graphics Processing Unit (GPU) is a parallel computing coprocessor
specialized in accelerating vector operations such as graphics rendering. The
General Purpose computing on Graphics Processing Unit methodology, or GPGPU,
consists in providing accessible programming interfaces for languages such as C
and Python that enable the use of GPUs in different parallel computing domains.
The Compute Unified Device Architecture (CUDA) is a GPGPU platform introduced
by the NVIDIA corporation.

The objective of this case study was to measure the performance improvements
that a general-purpose autotuning strategy would achieve for CUDA compiler
parameters. We implemented an autotuner for the CUDA compiler using the use the
OpenTuner framework~\cite{ansel2014opentuner} and used it to search for the
compilation parameters that optimize the performance of 17 heterogeneous GPU
applications, 12 of which are from the Rodinia Benchmark
Suite~\cite{che2009rodinia}.  We used 3 different NVIDIA GPUs in the
experiments, the Tesla K40, the GTX 980 and the GTX 750. We worked with
Hewlett-Packard Enterprise on this project.

The optimization achieved by autotuning often beat the compiler high-level
optimization options, such as \texttt{-O1}, \texttt{-O2} and \texttt{-O3}.  The
autotuner found compilation options that achieved over 2x speedup for the
\textit{Gaussian Elimination} problem from the Rodinia Benchmark Suite, almost
2x speedup for \emph{Heart Wall} problem, also from Rodinia, and over 4x
speedup for one of the matrix multiplication optimizations in our benchmark, in
comparison with the high-level compiler optimizations.  We also show that the
compilation parameters that optimize an algorithm for a given GPU architecture
will not always achieve the same performance in different hardware.

\subsection{Small Measurement Time}
\label{subsec:smalltime}

In the larger context of this qualifying work the most important characteristic
of this autotuning experiment is the short time it takes to compile and run
each application in the benchmark. Most applications, especially the ones with
smaller input sizes, had a \textit{small measurement time}.

\begin{figure}[htpb]
    \centering
    \includegraphics[width=.65\textwidth]{./images/overview_gpus}
    \caption{Autotuner representation and time scale of our experiments}
    \label{fig:overview-gpus}
\end{figure}

Figure~\ref{fig:overview-gpus} shows a representation of the autotuner
implemented in this study, and illustrates the time scale of our experiments.
The autotuner, in light blue, receives as input a Rodinia application, a target
GPU, input data for the application in the order of Gigabytes and a search
space composed of NVCC flags. The autotuner outputs a flag selection subject to
its input application and GPU.

Since in this study it was fast to compile and measure a program's performance,
an autotuner iteration is shown in Figure~\ref{fig:overview-gpus} to last in
the order of seconds. We could therefore test thousands of flag combinations
per hour in a sequential autotuner, exploring enough of the search space to
achieve the speedups reported.

The tool we used to implement our autotuner was not able to run measurements in
distributed environments. Therefore our autotuner could not deploy measurements
of different configurations on identical GPUs on the same network. An autotuner
with such capability would enable the exploration of more configurations in the
same time.

\subsection{Background}

\subsubsection{NVIDIA GPU Microarchitecture}
\label{sec:GPUsCUDA}

NVIDIA GPU architectures have multiple asynchronous and parallel Streaming
Multiprocessors (SMs) which contain Scalar Processors (SPs), Special Function
Units (SFUs) and load/store units. Each group of 32 parallel threads scheduled
by and SM, or \textit{warp}, is able to read from memory concurrently.  The
Tesla, Fermi, Kepler and Maxwell NVIDIA architectures vary in a large number of
features, such as number of cores, registers, SFUs, load/store units, on-chip
and cache memory sizes, processor clock frequency, memory bandwidth, unified
memory spaces and dynamic kernel launches.  Those differences are summarized in
the Compute Capability (C.C.) of an NVIDIA GPU.

The hierarchical memory of an NVIDIA GPU contains global and shared portions.
Global memory is big, off-chip, has a high latency and can be accessed by all
threads of the kernel.  Shared memory is small, on-chip, has a low-latency and
can be accessed only by threads in a same SM.  Each SM has its own shared L1
cache, and new architectures have coherent global L2 caches.  Optimizing thread
accesses to different memory levels is essential to achieve good performance.

\subsubsection{Compute Unified Device Architecture (CUDA)}

The CUDA programming model and platform enables the use of NVIDIA GPUs for
scientific and general purpose computation.  A single \textit{master} thread
runs in the CPU, launching and managing computations on the GPU.  Data for the
computations has to be transferred from the main memory to the GPU's memory.
Multiple computations launched by the master thread, or \textit{kernels}, can
run asynchronously and concurrently. If the threads from a same warp must
execute different instructions the CUDA compiler must generate code to branch
the execution correctly, making the program lose performance due to this
\textit{warp divergence}.

The CUDA language extends C and provides a multi-step compiler, called
\textit{NVCC}, that translates CUDA code to Parallel Thread Execution code, or
\textit{PTX}.  \textit{NVCC} uses the host's C++ compiler in several
compilation steps, and also to generate code to be executed in the host. The
final binary generated by \textit{NVCC} contains code for the GPU and the host.
When \textit{PTX} code is loaded by an application at run-time, it is compiled
to binary code by the host's device driver.  This binary code can be executed
in the device's processing cores, and is architecture-specific.  The targeted
architecture can be specified using \textit{NVCC} parameters.

\subsubsection{GPU Performance Models and Autotuning}

The accuracy of a GPU performance model is subject to low level elements such
as instruction pipeline usage and small cache hierarchies. A GPU's performance
approaches its peak when the instruction pipeline is saturated, but becomes
unpredictable when the pipeline is
under-utilized~\cite{zhang2011quantitative,amaris2015simple}.  Considering the
effects of small cache hierarchies~\cite{dao2015performance,picchi2015impact}
and memory-access
divergence~\cite{sampaio2013divergence,baghsorkhi2010adaptive} is also critical
to a GPU performance model.

Guo and Wang~\cite{guo2010auto} introduce a framework for autotuning
the number of threads and the sizes of blocks and warps used by the CUDA
compiler for sparse matrix and vector multiplication GPU applications.  Li
\emph{et al.}~\cite{li2009note} discuss the performance of autotuning
techniques in highly-tuned GPU General Matrix to Matrix Multiplication (GEMMs)
routines, highlighting the difficulty in developing optimized code for new GPU
architectures.  Grauer-Gray \emph{et al.}~\cite{grauer2012auto}
autotune an optimization space of GPU kernels focusing on tiling, loop
permutation, unrolling, and parallelization.  Chaparala \emph{et
al.}~\cite{chaparala2015autotuning} autotune GPU-accelerated Quadratic
Assignment Problem solvers.  Sedaghati \emph{et
al.}~\cite{sedaghati2015automatic} build a decision model for the selection of
sparse matrix representations in GPUs.

\subsection{Autotuner \& Search Space}
\label{sec:tunersearch}

This section discusses our autotuner implementation and the
search space of NVCC compiler parameters.

\subsubsection{Autotuner}

The autotuner we implemented used the \textit{multi-armed bandit with sliding
window, area under the curve credit assignment} meta-technique, simply named
\textit{AUC Bandit}.  AUC Bandit is OpenTuner's core
meta-technique~\cite{ansel2014opentuner}, and its ensemble of search techniques
is composed by implementations of the Nelder-Mead algorithm and three
variations of genetic algorithms.  Figure~\ref{fig:gpu-stack} shows a
simplified version of the steps necessary to generate the object code that will
be measured later.

The code for our autotuner and all the experiments and results is
available\footnote{Hosted at GitHub:
\url{https://github.com/phrb/gpu-autotuning} [Accessed on 10 February 2015]}
under the GNU General Public License.

\begin{figure}[htpb]
    \centering
    \includegraphics[width=.4\textwidth]{./images/gpu-stack}
    \caption{Simplified view of NVCC compilation}
    \label{fig:gpu-stack}
\end{figure}

\subsubsection{Search Space}

Table~\ref{tab:flags} details the subset of the CUDA configuration parameters
used in the experiments~\footnote{Adapted from:
\url{http://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc} [Accessed on 10
February 2015]}.  The parameters target different compilation steps: the
\emph{PTX} optimizing assembler; the \emph{NVLINK} linker; and the \emph{NVCC}
compiler.  We compared the performance of programs generated by tuned
parameters with the standard compiler optimizations, namely
\texttt{--opt-level=0,1,2,3}.  Different \texttt{--opt-level}s could also be
selected during tuning.  We did not use compiler options that target the host
linker or the library manager since they do not affect performance.  The size
of the search space defined by all possible combinations of the flags in
Table~\ref{tab:flags} is in the order of $10^{6}$ making hand-optimization or
exhaustive searches very time consuming.

\begin{table}[htpb]
    \centering
    \tiny
        \begin{tabular}{lp{0.6\textwidth}}
        \toprule
        \textbf{Flag}&\textbf{Description} \\\midrule
            \addlinespace{}
        \texttt{no-align-double} & Specifies that \texttt{malign-double} should not be passed as a compiler argument on 32-bit platforms. \textbf{Step}: NVCC \\
            \addlinespace{}
        \texttt{use\_fast\_math} & Uses the fast math library, implies \texttt{ftz=true}, \texttt{prec-div=false}, \texttt{prec-sqrt=false} and \texttt{fmad=true}. \textbf{Step}: NVCC \\
            \addlinespace{}
        \texttt{gpu-architecture} & Specifies the NVIDIA virtual GPU architecture for which the CUDA input files must be compiled. \textbf{Step}: NVCC \textbf{Values}: \texttt{sm\_20}, \texttt{sm\_21}, \texttt{sm\_30}, \texttt{sm\_32}, \texttt{sm\_35}, \texttt{sm\_50}, \texttt{sm\_52} \\
            \addlinespace{}
        \texttt{relocatable-device-code} & Enables the generation of relocatable device code. If disabled, executable device code is generated. Relocatable device code must be linked before it can be executed. \textbf{Step}: NVCC \\
            \addlinespace{}
        \texttt{ftz} & Controls single-precision denormals support. \texttt{ftz=true} flushes denormal values to zero and \texttt{ftz=false} preserves denormal values. \textbf{Step}: NVCC \\
            \addlinespace{}
        \texttt{prec-div} & Controls single-precision floating-point division and reciprocals. \texttt{prec-div=true} enables the IEEE round-to-nearest mode and \texttt{prec-div=false} enables the fast approximation mode. \textbf{Step}: NVCC \\
            \addlinespace{}
        \texttt{prec-sqrt} & Controls single-precision floating-point squre root. \texttt{prec-sqrt=true} enables the IEEE round-to-nearest mode and \texttt{prec-sqrt=false} enables the fast approximation mode. \textbf{Step}: NVCC \\
            \addlinespace{}
        \texttt{def-load-cache} & Default cache modifier on global/generic load. \textbf{Step}: PTX \textbf{Values}: \texttt{ca}, \texttt{cg}, \texttt{cv}, \texttt{cs} \\
            \addlinespace{}
        \texttt{opt-level} & Specifies high-level optimizations. \textbf{Step}: PTX \textbf{Values}: \texttt{0 - 3} \\
            \addlinespace{}
        \texttt{fmad} & Enables the contraction of floating-point multiplies and adds/subtracts into floating-point multiply-add operations (FMAD, FFMA, or DFMA). \textbf{Step}: PTX \\
            \addlinespace{}
        \texttt{allow-expensive-optimizations} & Enables the compiler to perform expensive optimizations using maximum available resources (memory and compile-time). If unspecified, default behavior is to enable this feature for optimization level $\geqslant$O2. \textbf{Step}: PTX \\
            \addlinespace{}
        \texttt{maxrregcount} & Specifies the maximum number of registers that GPU functions can use. \textbf{Step}: PTX \textbf{Values}: \texttt{16 - 64} \\
            \addlinespace{}
        \texttt{preserve-relocs} & Makes the \texttt{PTX} assembler generate relocatable references for variables and preserve relocations generated for them in the linked executable. \textbf{Step}: NVLINK \\
            \bottomrule
        \end{tabular}
    \caption{Description of flags in the search space}
    \label{tab:flags}
\end{table}

\subsection{Experiments}

This section presents the GPU testbed, the algorithm benchmark, the
autotuner implementation and its search space.

\subsubsection{GPU Testbed}

To be able to show that different GPUs require different options to improve
performance, and that it is possible to achieve speedups in different hardware,
we wanted to tune our benchmark for different NVIDIA microarchitectures.
Table~\ref{tab:GPUs} summarizes the hardware characteristics of the three GPUs.

\begin{table}[thpb]
    \centering
    \footnotesize
    \begin{tabular}{llllllll}
        \toprule
        \textbf{Model}&\textbf{C.C.}&\textbf{Global Memory}&\textbf{Bus}&\textbf{Bandwidth}&\textbf{L2}&\textbf{Cores/SM}&\textbf{Clock} \\ \midrule
        Tesla-K40&3.5&12 GB&384-bit&276.5 GB/s&1.5 MB&2880/15&745 Mhz \\
        GTX-750&5.0&1 GB&128-bit&86.4 GB/s&2 MB&512/4&1110 Mhz \\
        GTX-980&5.2&4 GB&256-bit&224.3 GB/s&2 MB&2048/16&1216 Mhz \\ \bottomrule
    \end{tabular}
    \caption{Hardware specifications of the GPUs in the testbed}
    \label{tab:GPUs}
\end{table}

\subsubsection{Algorithm Benchmark}

We composed a benchmark with 17 heterogeneous GPU applications.  The benchmark
contains 4 optimization strategies for \emph{matrix multiplication} (counted as
a single application), 1 vector addition problem, 1 solution for the
\emph{maximum sub-array problem}~\cite{ferreira2014parallel}, 2 \emph{sorting}
algorithms and 12 applications from the Rodinia Benchmark
Suite~\cite{che2009rodinia}.

\begin{table}[htpb]
    \centering
    \footnotesize
        \begin{tabular}{lll}
            \toprule
            \textbf{Application} & \textbf{Berkeley Dwarf\cite{asanovic2009view}} & \textbf{Domain} \\\midrule
            B+Tree (BPT) & Graph Traversal& Search \\
            Back Propagation (BCK) & Unstructured Grid & Pattern Recognition \\
            Breadth-First Search (BFS) & Graph Traversal & Graph Algorithms \\
            Gaussian Elimination (GAU) & Dense Linear Algebra & Linear Algebra \\
            Heart Wall (HWL) & Structured Grid & Medical Imaging \\
            Hot Spot (HOT) & Structured Grid & Physics Simulation \\
            K-Means (KMN) & Dense Linear Algebra & Data Mining \\
            LavaMD (LMD) & N-Body & Molecular Dynamics \\
            LU Decomposition (LUD) & Dense Linear Algebra & Linear Algebra \\
            Myocyte (MYO) & Structured Grid & Biological Simulation \\
            Needleman-Wunsch (NDL) & Dynamic Programming & Bioinformatics \\
            Path Finder (PTF) & Dynamic Programming & Grid Traversal \\\bottomrule
        \end{tabular}
    \caption{Rodinia applications used in the experiments}
    \label{tab:Rodinia}
\end{table}

Table~\ref{tab:Rodinia} shows the Rodinia applications contained in our
benchmark and their correspondent three-letter code.  The other applications in
the benchmark were CUDA implementations of the following:

\begin{itemize}
    \item Matrix multiplications using:
        \begin{itemize}
             \item Global memory with non-coalesced accesses (MMU)
             \item Global memory with coalesced accesses (MMG)
             \item Shared memory with non-coalesced accesses to global memory
                 (MSU)
             \item Shared memory with coalesced accesses to global memory (MMS)
        \end{itemize}
    \item Simple vector addition algorithm (VAD)
    \item Solution~\cite{alves2004bsp,ferreira2014parallel} for the Maximum
        Sub-Array Problem (MSA)
    \item Quicksort (QKS) and Bitonicsort (BTN)~\footnote{Obtained from:
        \url{http://digitalcommons.providence.edu/student\_scholarship/7/}
        [Accessed on 10 February 2015]}
\end{itemize}


\subsection{Results}
\label{sec:GPUresults}

This section presents the speedups achieved for all algorithms in the
application benchmark, highlights the most significant speedups, and discusses
the performance and accuracy of the autotuner.

\subsubsection{Performance Improvements}

All boxplots presented in this section were made using the standard
implementations available for the R language.  The black band inside the boxes
represents the median of the measurements. The lower and upper bounds of the
box represent, respectively, the first and third quartiles of the data. The
whiskers represent the third and first quartile plus and minus the \emph{inner
quartile range} times 1.5. Finally, the circles represent the outliers.

\begin{figure}[htpb]
    \centering
    \begin{minipage}{.48\textwidth}
        \centering
        \includegraphics[scale=.22]{./images/heartwall-0-Tesla-K40-Box.eps}
        \caption{Boxplots for the Tesla K40, comparing autotuned results and high-level compiler optimizations for the Heart Wall problem (HWL)}
        \label{fig:K40hwl}
    \end{minipage}%
    \hfill
    \begin{minipage}{.48\textwidth}
        \centering
        \includegraphics[scale=.22]{./images/gaussian-0-GTX-980-Box.eps}
        \caption{Boxplots for the GTX 980 GPU, comparing autotuned results and high-level compiler optimizations for the Gaussian Elimination problem (GAU)}
        \label{fig:980gau}
    \end{minipage}%
\end{figure}

Figures~\ref{fig:K40hwl}, \ref{fig:980gau}, \ref{fig:K40ptf} and
\ref{fig:K40myo} compare the distributions of 10 perfomance measurements of
binaries compiled with high-level compiler optimizations and with autotuned
compiler options. The results for the high-level optimizations
\texttt{--opt-level=0,1,2,3} were denoted by \emph{-O0}, \emph{-O1}, \emph{-O2}
and \emph{-O3}.  The results for the autotuned configurations were denoted by
the keyword \emph{Tuned}.

\begin{figure}[htpb]
    \centering
    \begin{minipage}{.48\textwidth}
        \centering
        \includegraphics[scale=.22]{./images/Pathfinder-0-Tesla-K40-Box.eps}
        \caption{Boxplots for the Tesla K40, comparing autotuned results and high-level compiler optimizations for the Path Finder problem (PTF)}
        \label{fig:K40ptf}
    \end{minipage}%
    \hfill
    \begin{minipage}{.48\textwidth}
        \centering
        \includegraphics[scale=.22]{./images/myocyte-0-Tesla-K40-Box.eps}
        \caption{Boxplots for the Tesla K40, comparing autotuned results and high-level compiler optimizations for the Myocyte problem (MYO)}
        \label{fig:K40myo}
    \end{minipage}%
\end{figure}

Figure~\ref{fig:K40hwl} shows that the autotuned solution for the Heart Wall
problem (HWL) in the Tesla K40 achieved over 2x speedup in comparison with the
high-level CUDA optimizations.  Figure~\ref{fig:980gau} shows, in the GTX 980,
almost 2.5x speedup for the Gaussian Elimination problem (GAU).
Figure~\ref{fig:K40ptf} show 10\% speedup of the autotuned solution for the
Path Finder problem (PTF) in the Tesla K40.  Figure~\ref{fig:K40myo} shows over
5\% speedup of the autotuned solution, also in the Tesla K40, for the Myocyte
problem (MYO).

\begin{figure}[htpb]
    \centering
    \begin{minipage}{.48\textwidth}
        \centering
        \includegraphics[scale=.15]{./images/MatrixSummary.eps}
        \caption{Summary of the speedups achieved versus \emph{-O2} in the matrix multiplication optimizations}
        \label{fig:matrixsummary}
    \end{minipage}%
    \hfill
    \begin{minipage}{.48\textwidth}
        \centering
        \includegraphics[scale=.15]{./images/Summary.eps}
        \caption{Summary of the speedups achieved versus \emph{-O2} in the other independent applications}
        \label{fig:summary}
    \end{minipage}%
\end{figure}

Figures~\ref{fig:matrixsummary}, \ref{fig:summary}, \ref{fig:rodiniasummary}
and \ref{fig:rodiniasummarysmall} present summaries of the results. The
autotuner did not find solutions that improved upon the high-level
optimizations for the problems BTN and MSU in any of the GPUs of the testbed,
but it found solutions that achieved speedups for at least one GPU for the
other problems.

\begin{figure}[htpb]
    \centering
    \begin{minipage}{.48\textwidth}
        \centering
        \includegraphics[scale=.15]{./images/RodiniaSummary.eps}
        \caption{Summary of the biggest speedups achieved versus \emph{-O2} in the Rodinia applications}
        \label{fig:rodiniasummary}
    \end{minipage}%
    \hfill
    \begin{minipage}{.48\textwidth}
        \centering
        \includegraphics[scale=.15]{./images/RodiniaSummary_small.eps}
        \caption{Summary of the smaller speedups achieved versus \emph{-O2} in the Rodinia applications}
        \label{fig:rodiniasummarysmall}
    \end{minipage}%
\end{figure}

We do not know precisely which hardware characteristics impacted performance
the most.  Although more experiments are needed to confirm the following
hypothesis, we believe that the Maxwell GPUs, the GTX 980 and GTX 750, had
differing results from the Tesla K40 because they are consumer grade GPUs,
producing not so precise results and with different default optimizations. The
similarities between the compute capabilities of the GTX GPUs could also
explain the observed differences from the K40. Finally, the overall greater
computing power of the GTX 980 could explain its differing results from the GTX
750, since the GTX 980 has a greater number of Cores, SMs/Cores, Bandwidth,
Bus, Clock and Global Memory.

\subsubsection{Autotuner Performance}

This section presents an assessment of the autotuner's performance.
Figures~\ref{fig:K40hwBest}, \ref{fig:980gauBest}, \ref{fig:K40ptfBest} and
\ref{fig:K40myoBest} present the performance of the best solution found by the
autotuner versus the time these solutions were found, in seconds since the
beginning of the tuning process. The Figures show the evolution of the
autotuned solution for the best results in our experiments, the Heart Wall
problem (HWL) and the Gaussian Elimination problem (GAU) in the GTX 980, and
the Path Finder problem (PTF) and the Myocyte problem (MYO) in the Tesla K40.

\begin{figure}[htpb]
    \centering
    \begin{minipage}{.48\textwidth}
        \centering
        \includegraphics[scale=.22]{./images/heartwall-0-Tesla-K40-Best.eps}
        \caption{Best solutions found by the autotuner over time for the Heart Wall problem (HWL) in the Tesla K40}
        \label{fig:K40hwBest}
    \end{minipage}%
    \hfill
    \begin{minipage}{.48\textwidth}
        \centering
        \includegraphics[scale=.22]{./images/gaussian-0-GTX-980-Best.eps}
        \caption{Best solutions found by the autotuner over time for the Gaussian Elimination problem (GAU) in the GTX 980}
        \label{fig:980gauBest}
    \end{minipage}%
\end{figure}

The points in each graph represent the performance, in the y-axis, of the best
configuration found at the corresponding tuning time, shown in the x-axis. The
leftmost point in each graph represents the performance of a configuration
chosen at random by the autotuner. Each subsequent point represents the
performance of the best configuration found so far.

Note that the autotuner is able to quickly improve upon the initial random
configuration, but the rate of improvement decays over tuning time. The
duration of all tuning runs was two hours, or 7200 seconds.  The rightmost
point in each graph represents the performance of the last improving
configuration found by the autotuner. After the tuning time of this
configuration, the autotuner did not find any improving configuration until the
end of its tuning run.

\begin{figure}[htpb]
    \centering
    \begin{minipage}{.48\textwidth}
        \centering
        \includegraphics[scale=.22]{./images/Pathfinder-0-Tesla-K40-Best.eps}
        \caption{Best solutions found by the autotuner over time for the Path Finder problem (PTF) in the Tesla K40}
        \label{fig:K40ptfBest}
    \end{minipage}%
    \hfill
    \begin{minipage}{.48\textwidth}
        \centering
        \includegraphics[scale=.22]{./images/myocyte-0-Tesla-K40-Best.eps}
        \caption{Best solutions found by the autotuner over time for the Myocyte problem (MYO) in the Tesla K40}
        \label{fig:K40myoBest}
    \end{minipage}
\end{figure}

The autotuner used the mean of 10 performance measurements of a given
configuration as the fitness value, reducing fluctuations and improving the
measurement's accuracy. This is reflected in the proximity of the measurements
reported by the autotuner during tuning, shown in figures~\ref{fig:K40hwBest},
\ref{fig:980gauBest}, \ref{fig:K40ptfBest} and \ref{fig:K40myoBest} and the
mean values of 10 measurements of the final solution, shown in the
corresponding figures~\ref{fig:K40hwl}, \ref{fig:980gau}, \ref{fig:K40ptf} and
\ref{fig:K40myo}.

\subsubsection{Parameter Selection}

\begin{table}[htpb]
    \centering
    \footnotesize
    \begin{tabular}{lll}
        \toprule
        \textbf{Flag} & \textbf{Cluster 0 (17\%)} & \textbf{Cluster 1 (83\%)} \\\midrule
        \texttt{no-align-double}               & on     & on     \\
        \texttt{use\_fast\_math}               & on     & on     \\
        \texttt{preserve-relocs}               & off    & off    \\
        \texttt{relocatable-device-code}       & true   & false  \\
        \texttt{ftz}                           & true   & true   \\
        \texttt{prec-div}                      & true   & false  \\
        \texttt{prec-sqrt}                     & true   & true   \\
        \texttt{fmad}                          & false  & false  \\
        \texttt{allow-expensive-optimizations} & false  & true   \\
        \texttt{gpu-architecture}              & sm\_20 & sm\_50 \\
        \texttt{def-load-cache}                & cv     & ca     \\
        \texttt{opt-level}                     & 1      & 3      \\
        \texttt{maxrregcount}                  & 42     & 44.6   \\\bottomrule
        \end{tabular}
    \caption{Parameter clusters for all Rodinia problems in the GTX 750}
    \label{tab:750RodiniaClusters}
\end{table}

We attempted to associate compilation parameters to applications and GPUs using
WEKA's~\cite{holmes1994weka} clustering algorithms. Although we could not find
significant relations for most applications we detected that the
\texttt{ftz=true} in MMS and the Compute Capabilities 3.0, 5.0 and 5.2 in GAU
caused the speedups observed in the GTX 980 for these applications.
Table~\ref{tab:750RodiniaClusters} shows clusters obtained with the K-means
WEKA algorithm for autotuned parameter sets for the Rodinia Benchmark in the
GTX 750. Unlike most clusters found for all GPUs and problems, these clusters
did not contain an equal number of instances. The difficulty of finding
associations between compiler optimizations, applications and GPUs justifies
the autotuning of compiler parameters in the general case.

\subsection{Summary}
\label{subsec:GPUconcl}

This study showed that it is possible to improve the performance of GPU
applications applying empirical and automatic tuning techniques. Our results
and clustering attempts emphasize the importance of automatic optimization
techniques by showing that different compilation options have to be selected in
order to achieve performance improvements in different GPUs and that it is
difficult to associate compiler optimization parameters, applications and GPUs.

An interesting direction for future studies is including application parameters
and options for the GCC compiler, which also composes the CUDA compilation
chain. Section~\ref{sec:stochastic-gpu-tuner} will discuss our efforts on spreading the autotuning
of CUDA compiler parameters to the NVIDIA CUDA programmer community.
